# original_rag.py  (í˜¹ì€ ì‚¬ìš© ì¤‘ì¸ íŒŒì¼ëª…)
import os
import re
import json
import hashlib
import pickle
from tqdm import tqdm
from pathlib import Path
from collections import defaultdict
from typing import TypedDict, Optional

import numpy as np
from dotenv import load_dotenv

from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.retrievers import BM25Retriever
from langchain.docstore.document import Document
from sentence_transformers import CrossEncoder
from langgraph.graph import StateGraph, END
from langchain_openai import ChatOpenAI
from langchain.schema import SystemMessage, HumanMessage



_PUNCT = re.compile(r"[ \t\r\n\-_()/\[\]{}Â·.,!?'\"â€¦]+")

def _normalize_name(s: str) -> str:
    """íŒŒì¼ëª…/ì¹´ë“œëª…ì„ ë¹„êµí•˜ê¸° ìœ„í•œ ì •ê·œí™”: ê³µë°±/êµ¬ë‘ì  ì œê±° + ì†Œë¬¸ì."""
    if not s:
        return ""
    s = s.strip()
    # í™•ì¥ì/ìì£¼ ë¶™ëŠ” ì ‘ë¯¸ì–´ ì œê±°
    s = re.sub(r"\.json$", "", s, flags=re.IGNORECASE)
    for suf in ("_ì •ì œ", "_ìµœì¢…", "_clean", "_final"):
        if s.endswith(suf):
            s = s[: -len(suf)]
    return _PUNCT.sub("", s).lower()


class GeneratorState(TypedDict):
    card_name: str
    user_question: str
    context_chunks: list[str]
    prompt: str
    answer: str
    simplified_answer: str
    explain_easy: bool


class FAISSRAGRetriever:
    """Original RAG ì‹œìŠ¤í…œ - ì¹´ë“œë³„ ìƒì„¸ ì •ë³´ ê²€ìƒ‰ ë° ì§ˆì˜ì‘ë‹µ"""

    def __init__(self):
        os.environ["KMP_DUPLICATE_LIB_OK"] = "TRUE"
        load_dotenv()

        # === í”„ë¡œì íŠ¸ ê¸°ì¤€ ê²½ë¡œ ì„¤ì • ===
        proj: Path = Path(__file__).resolve().parent        # fastapi_project í´ë”
        self.current_file_dir = str(proj)                   # ë¬¸ìì—´ ê²½ë¡œë„ ë³´ê´€
        self.base_path = str(proj)                          # ê³¼ê±° í˜¸í™˜

        def pick_dir(label: str, candidates: list[Path]) -> Optional[str]:
            for p in candidates:
                if p.exists():
                    print(f"ğŸ“‚ {label} ë””ë ‰í† ë¦¬: {p}")
                    return str(p)
            tried = "\n  - " + "\n  - ".join(map(str, candidates))
            print(f"âš ï¸ {label} ë””ë ‰í† ë¦¬ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ì‹œë„í•œ ê²½ë¡œ:{tried}")
            return None

        # ì‹ ìš©/ì²´í¬ ì¹´ë“œ ë£¨íŠ¸ ë””ë ‰í† ë¦¬ ìë™ íƒìƒ‰ (ìš°ì„ ìˆœìœ„ ìˆœ)
        self.credit_dir = pick_dir(
            "ì‹ ìš©ì¹´ë“œ",
            [
                proj / "ì‹ ìš©ì¹´ë“œ",
                proj / "JSON" / "ì‹ ìš©json",
                proj.parent / "ì‹ ìš©ì¹´ë“œ",
            ],
        )
        self.check_dir = pick_dir(
            "ì²´í¬ì¹´ë“œ",
            [
                proj / "ì²´í¬ì¹´ë“œ",
                proj / "JSON" / "ì²´í¬json",
                proj.parent / "ì²´í¬ì¹´ë“œ",
            ],
        )

        # ì´í›„ ìˆœíšŒí•  ë£¨íŠ¸ ëª©ë¡
        self.data_dirs = [d for d in [self.credit_dir, self.check_dir] if d]
        if not self.data_dirs:
            raise FileNotFoundError("ì‹ ìš©ì¹´ë“œ/ì²´í¬ì¹´ë“œ ë°ì´í„° ë””ë ‰í† ë¦¬ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")

        # selected_cards.json ìœ„ì¹˜
        self.selected_cards_path = os.path.join(self.current_file_dir, "selected_cards.json")

        # ì„ë² ë”© ì €ì¥ ë””ë ‰í† ë¦¬
        self.embeddings_dir = os.path.join(self.current_file_dir, "original_embeddings")
        os.makedirs(self.embeddings_dir, exist_ok=True)
        print(f"ğŸ“ ì„ë² ë”© ì €ì¥ ë””ë ‰í† ë¦¬: {self.embeddings_dir}")

        # ëª¨ë¸ë“¤
        self.llm = ChatOpenAI(model="gpt-4o", temperature=0.3)
        self.embedding_model = HuggingFaceEmbeddings(
            model_name="BAAI/bge-m3",
            model_kwargs={"device": "cpu"},
        )
        self.reranker = CrossEncoder("cross-encoder/ms-marco-MiniLM-L-6-v2")

        # LangGraph
        self.graph = self._build_langgraph()

        # ìºì‹œ
        self._document_cache: dict[str, list[Document]] = {}
        self._faiss_cache: dict[str, FAISS] = {}
        self._bm25_cache: dict[str, BM25Retriever] = {}

        print("ğŸ‰ FAISSRAGRetriever ì´ˆê¸°í™” ì™„ë£Œ!")

    # ----------------- ìœ í‹¸/ë¡œë”© -----------------
    def get_latest_card_from_selected_cards(self) -> Optional[str]:
        try:
            if not os.path.exists(self.selected_cards_path):
                print(f"âŒ selected_cards.json íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {self.selected_cards_path}")
                return None

            with open(self.selected_cards_path, "r", encoding="utf-8") as f:
                data = json.load(f)
            if not data:
                print("âŒ selected_cards.jsonì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤")
                return None

            last_card = data[-1]
            card_name = last_card.get("card_name")
            print("ğŸ¯ ë§ˆì§€ë§‰ ì¹´ë“œ ì •ë³´:")
            print(f"   - Card Name: {card_name}")
            print(f"   - Timestamp: {last_card.get('timestamp', 'N/A')}")
            print(f"   - Question: {last_card.get('question', 'N/A')}")
            print(f"   - Card Type: {last_card.get('card_type', 'N/A')}")
            print(f"   - Keyword: {last_card.get('keyword', 'N/A')}")
            return card_name
        except Exception as e:
            print(f"âŒ selected_cards.json ì½ê¸° ì¤‘ ì˜¤ë¥˜: {e}")
            return None

    def load_documents_field_level(self, json_paths: list[str]) -> list[Document]:
        print(f"ğŸ“‹ ë¬¸ì„œ ë¡œë”© ì‹œì‘... (ì´ {len(json_paths)}ê°œ íŒŒì¼)")
        documents: list[Document] = []

        for json_path in tqdm(json_paths, desc="ğŸ“‹ ë¬¸ì„œ ë¡œë”© ì¤‘"):
            try:
                with open(json_path, "r", encoding="utf-8") as f:
                    data = json.load(f)
                card_name = data.get("card_name", "UnknownCard")

                for section in data.get("sections", []):
                    heading = section.get("heading", "")
                    subheading = section.get("subheading", "")

                    for key in [
                        "benefit",
                        "benefits",
                        "fee",
                        "agreement",
                        "condition",
                        "conditions",
                        "etc",
                        "overseas_usage",
                    ]:
                        val = section.get(key)
                        if not val:
                            continue
                        contents = val if isinstance(val, list) else [val]
                        merged_text = "\n".join([c.strip() for c in contents if len(c.strip()) > 0])
                        if len(merged_text.strip()) < 10:
                            continue

                        full_text = f"[{card_name}]\n{heading} - {subheading}\n<{key}>\n{merged_text}"
                        documents.append(
                            Document(
                                page_content=full_text,
                                metadata={
                                    "card_name": card_name,
                                    "field": key,
                                    "heading": heading,
                                    "subheading": subheading,
                                },
                            )
                        )
            except Exception as e:
                print(f"â— Error reading {json_path}: {e}")

        return documents

    def _get_card_category_from_path(self, json_path: str) -> str:
        p = json_path.lower()
        # í•œêµ­ì–´ í´ë”ëª…ê³¼ ì˜ë¬¸ í‚¤ì›Œë“œ ëª¨ë‘ ëŒ€ì‘
        if ("ì‹ ìš©" in p) or ("credit" in p):
            return "credit"
        if ("ì²´í¬" in p) or ("check" in p):
            return "check"
        return "unknown"

    def _load_category_documents(self, category: str) -> list[Document]:
        """ì¹´í…Œê³ ë¦¬ë³„ ëª¨ë“  ë¬¸ì„œ ë¡œë”© (í•˜ìœ„í´ë” ì¬ê·€ íƒìƒ‰)"""
        print(f"ğŸ“‹ {category.upper()} ì¹´ë“œ ì¹´í…Œê³ ë¦¬ ë¬¸ì„œ ë¡œë”© ì¤‘...")

        root_dir = self.credit_dir if category == "credit" else self.check_dir if category == "check" else None
        if not root_dir or not os.path.exists(root_dir):
            print(f"âŒ ì¹´í…Œê³ ë¦¬ ë””ë ‰í† ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {root_dir}")
            return []

        json_files = [str(p) for p in Path(root_dir).rglob("*.json")]
        print(f"ğŸ“Š {category.upper()} ì¹´í…Œê³ ë¦¬ì—ì„œ {len(json_files)}ê°œ JSON íŒŒì¼ ë°œê²¬")

        if not json_files:
            print(f"âš ï¸ {category.upper()} ì¹´í…Œê³ ë¦¬ì— JSON íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤")
            return []

        return self.load_documents_field_level(json_files)

    def _save_category_embeddings(self, category: str, documents: list[Document], faiss_index: FAISS) -> bool:
        print(f"ğŸ’¾ {category.upper()} ì¹´ë“œ ì„ë² ë”© ì €ì¥ ì¤‘...")

        try:
            base_filename = f"{category}_card"
            faiss_path = os.path.join(self.embeddings_dir, f"{base_filename}_embeddings.faiss")
            pkl_path = os.path.join(self.embeddings_dir, f"{base_filename}_embedding_data.pkl")
            metadata_path = os.path.join(self.embeddings_dir, f"{base_filename}_metadata.json")
            texts_path = os.path.join(self.embeddings_dir, f"{base_filename}_texts.txt")

            print("   ğŸ“ ì €ì¥ ê²½ë¡œ:")
            print(f"      - FAISS ì¸ë±ìŠ¤: {faiss_path}")
            print(f"      - ì„ë² ë”© ë°ì´í„°: {pkl_path}")
            print(f"      - ë©”íƒ€ë°ì´í„°: {metadata_path}")
            print(f"      - í…ìŠ¤íŠ¸: {texts_path}")

            # 1) faiss raw index ì €ì¥
            if hasattr(faiss_index, "index"):
                import faiss as faiss_lib

                faiss_lib.write_index(faiss_index.index, faiss_path)

            # 2) ë¬¸ì„œ/í…ìŠ¤íŠ¸/ë©”íƒ€ ì €ì¥
            embedding_data = {
                "documents": documents,
                "texts": [doc.page_content for doc in documents],
                "metadatas": [doc.metadata for doc in documents],
                "embeddings": (
                    faiss_index.index.reconstruct_n(0, faiss_index.index.ntotal)
                    if hasattr(faiss_index.index, "reconstruct_n")
                    else None
                ),
            }
            with open(pkl_path, "wb") as f:
                pickle.dump(embedding_data, f)

            import datetime

            metadata_summary = {
                "category": category,
                "total_documents": len(documents),
                "cards": list({doc.metadata.get("card_name", "Unknown") for doc in documents}),
                "created_at": str(datetime.datetime.now().isoformat()),
                "embedding_model": "BAAI/bge-m3",
            }
            with open(metadata_path, "w", encoding="utf-8") as f:
                json.dump(metadata_summary, f, ensure_ascii=False, indent=2)

            with open(texts_path, "w", encoding="utf-8") as f:
                f.write(f"=== {category.upper()} ì¹´ë“œ ë¬¸ì„œ í…ìŠ¤íŠ¸ ===\n")
                f.write(f"ì´ ë¬¸ì„œ ìˆ˜: {len(documents)}\n")
                f.write(f"í¬í•¨ëœ ì¹´ë“œ: {', '.join(metadata_summary['cards'])}\n")
                f.write("=" * 50 + "\n\n")
                for i, doc in enumerate(documents):
                    f.write(f"ë¬¸ì„œ {i+1}:\n")
                    f.write(f"ì¹´ë“œëª…: {doc.metadata.get('card_name', 'Unknown')}\n")
                    f.write(f"í•„ë“œ: {doc.metadata.get('field', 'Unknown')}\n")
                    f.write(f"ì œëª©: {doc.metadata.get('heading', '')} - {doc.metadata.get('subheading', '')}\n")
                    f.write("-" * 30 + "\n")
                    body = doc.page_content
                    f.write(body[:500] + "...\n" if len(body) > 500 else body + "\n")
                    f.write("\n" + "=" * 50 + "\n\n")

            print(f"âœ… {category.upper()} ì¹´ë“œ ì„ë² ë”© ì €ì¥ ì„±ê³µ!")
            return True
        except Exception as e:
            print(f"âŒ {category.upper()} ì¹´ë“œ ì„ë² ë”© ì €ì¥ ì‹¤íŒ¨: {e}")
            import traceback

            traceback.print_exc()
            return False

    def _load_category_embeddings(self, category: str) -> tuple[Optional[list[Document]], Optional[FAISS], Optional[BM25Retriever]]:
        base_filename = f"{category}_card"
        faiss_path = os.path.join(self.embeddings_dir, f"{base_filename}_embeddings.faiss")
        pkl_path = os.path.join(self.embeddings_dir, f"{base_filename}_embedding_data.pkl")

        if not os.path.exists(pkl_path):
            print(f"âš ï¸ {category.upper()} ì¹´ë“œ ì„ë² ë”© íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {pkl_path}")
            return None, None, None

        try:
            with open(pkl_path, "rb") as f:
                embedding_data = pickle.load(f)
            documents: list[Document] = embedding_data["documents"]

            if os.path.exists(faiss_path):
                import faiss as faiss_lib

                index = faiss_lib.read_index(faiss_path)
                faiss_index = FAISS(
                    embedding_function=self.embedding_model,
                    index=index,
                    docstore={i: doc for i, doc in enumerate(documents)},
                    index_to_docstore_id={i: i for i in range(len(documents))},
                )
                print("   ğŸ” FAISS ì¸ë±ìŠ¤ ë¡œë“œ ì™„ë£Œ")
            else:
                print("   âš ï¸ FAISS íŒŒì¼ì´ ì—†ì–´ ì¬ìƒì„±í•©ë‹ˆë‹¤...")
                faiss_index = FAISS.from_documents(documents, self.embedding_model)

            bm25 = BM25Retriever.from_documents(documents)
            bm25.k = 60

            print(f"âœ… {category.upper()} ì¹´ë“œ ì„ë² ë”© ë¡œë“œ ì„±ê³µ!")
            return documents, faiss_index, bm25
        except Exception as e:
            print(f"âŒ {category.upper()} ì¹´ë“œ ì„ë² ë”© ë¡œë“œ ì‹¤íŒ¨: {e}")
            import traceback

            traceback.print_exc()
            return None, None, None

    def build_category_embeddings(self, force_rebuild: bool = False):
        for category in ["credit", "check"]:
            print("\n" + "=" * 50)
            base_filename = f"{category}_card"
            pkl_path = os.path.join(self.embeddings_dir, f"{base_filename}_embedding_data.pkl")

            if os.path.exists(pkl_path) and not force_rebuild:
                print(f"âœ… {category.upper()} ì¹´ë“œ ì„ë² ë”©ì´ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤: {pkl_path}")
                print("   force_rebuild=Trueë¡œ ì„¤ì •í•˜ë©´ ì¬ë¹Œë“œë©ë‹ˆë‹¤.")
                continue

            documents = self._load_category_documents(category)
            if not documents:
                print(f"âš ï¸ {category.upper()} ì¹´í…Œê³ ë¦¬ì— ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤. ìŠ¤í‚µí•©ë‹ˆë‹¤.")
                continue

            cards = list({doc.metadata.get("card_name", "Unknown") for doc in documents})
            print("ğŸ“Š í†µê³„:")
            print(f"   - ì´ ë¬¸ì„œ ìˆ˜: {len(documents)}")
            print(f"   - ì¹´ë“œ ìˆ˜: {len(cards)}")
            print(f"   - ìƒ˜í”Œ: {', '.join(cards[:5])}{'...' if len(cards) > 5 else ''}")

            print(f"ğŸ” {category.upper()} ì¹´ë“œ FAISS ì¸ë±ìŠ¤ ìƒì„± ì¤‘...")
            faiss_index = FAISS.from_documents(documents, self.embedding_model)
            print("âœ… ìƒì„± ì™„ë£Œ")

            if self._save_category_embeddings(category, documents, faiss_index):
                print(f"ğŸ‰ {category.upper()} ì¹´ë“œ ì„ë² ë”© ë¹Œë“œ ì™„ë£Œ!")
            else:
                print(f"âŒ {category.upper()} ì¹´ë“œ ì„ë² ë”© ë¹Œë“œ ì‹¤íŒ¨!")

        print(f"ğŸ“ ì €ì¥ ìœ„ì¹˜: {self.embeddings_dir}")

    # ----------------- ê²€ìƒ‰/ìƒì„± -----------------
    def reciprocal_rank_fusion(self, faiss_results: list, bm25_results: list, k: int = 60) -> list[str]:
        scores = defaultdict(float)

        def update_scores(results, weight):
            for rank, item in enumerate(results):
                key = item.page_content.strip()
                scores[key] += weight / (k + rank + 1)

        update_scores(faiss_results, weight=0.6)
        update_scores(bm25_results, weight=0.4)

        sorted_chunks = sorted(scores.items(), key=lambda x: x[1], reverse=True)
        return [chunk for chunk, _ in sorted_chunks[:k]]

    def build_generator_prompt(self, card_name: str, user_question: str, context_chunks: list[str]) -> str:
        context_text = "\n".join(context_chunks)
        return f"""
ë‹¹ì‹ ì€ ì‹ ìš©ì¹´ë“œ ë° ì²´í¬ì¹´ë“œ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ì ì§ˆë¬¸ì— ëŒ€í•´ **ë¬¸ì„œ ê¸°ë°˜ìœ¼ë¡œ êµ¬ì²´ì ì´ê³  ì •í™•í•œ ë‹µë³€ì„ ì œê³µí•˜ëŠ” AI ì „ë¬¸ê°€**ì…ë‹ˆë‹¤.

ì¹´ë“œ ì´ë¦„: {card_name}

ì•„ë˜ëŠ” í•´ë‹¹ ì¹´ë“œì˜ ì•½ê´€ ë° ìƒí’ˆì„¤ëª…ì„œì—ì„œ ì¶”ì¶œëœ ì¼ë¶€ ë¬¸ì„œ ë‚´ìš©ì…ë‹ˆë‹¤:

[ë¬¸ì„œ ë‚´ìš©]
{context_text}

[ì‚¬ìš©ì ì§ˆë¬¸]
{user_question}

[ì‘ë‹µ ì¡°ê±´]
1. ë°˜ë“œì‹œ **ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ë§ê²Œ ìœ„ ë¬¸ì„œ ë‚´ìš©ì— ê¸°ë°˜**í•˜ì—¬ ë‹µë³€í•˜ê³ , ê°€ëŠ¥í•œ í•œ **êµ¬ì²´ì ì´ê³  í’ë¶€í•œ ì„¤ëª…**ì„ ì œê³µí•˜ì„¸ìš”.
   (ì˜ˆ: í˜œíƒ ì—…ì¢…, í˜œíƒ ì¡°ê±´, ë¶€ê°€ í˜œíƒ, í•´ì§€ ì¡°ê±´ ë“±)
2. ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ë‹µë³€í•  ë•Œ **ì •í™•í•œ ìˆ˜ì¹˜, ì‹œê¸°, ì¡°ê±´, ë¬¸êµ¬**ë¥¼ ë°˜ì˜í•˜ì„¸ìš”.
   (ì˜ˆ: "ì›” ìµœëŒ€ 2íšŒ", "êµ­ë‚´ì „ìš© 9,000ì›", "ì „ì›” 30ë§Œì› ì´ìƒ ì´ìš© ì‹œ ì ìš©")
3. **ì¹´ë“œ ì•½ê´€ì˜ ì£¼ì˜ì‚¬í•­ì´ë‚˜ í™•ì¸ì‚¬í•­** (ì˜ˆ: ì „ì›” ì‹¤ì  ì œì™¸ ì¡°ê±´, ì†Œë¹„ì ê¶Œë¦¬, ì—°ì²´ ì‹œ ë¶ˆì´ìµ ë“±)ë„ ë¬¸ë§¥ì— ë”°ë¼ í¬í•¨í•´ ì£¼ì„¸ìš”.
4. ì‚¬ìš©ìì˜ ì§ˆë¬¸ì´ ì—¬ëŸ¬ í•­ëª©ì„ í¬í•¨í•  ê²½ìš°, ê° í•­ëª©ë³„ë¡œ **ì²´ê³„ì ìœ¼ë¡œ êµ¬ì„±í•˜ì—¬ ë‹µë³€**í•˜ì„¸ìš”.
5. ë¬¸ì„œì— ëª…ì‹œë˜ì§€ ì•Šì€ ë‚´ìš©ì€ "ë¬¸ì„œì— ëª…ì‹œëœ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤."ë¼ê³  ë‹µë³€í•˜ê³ , **ì ˆëŒ€ ê±°ì§“ ì •ë³´ë¥¼ ì§€ì–´ë‚´ì„œëŠ” ì•ˆë©ë‹ˆë‹¤.**
""".strip()

    def _build_prompt_node(self, state: GeneratorState) -> GeneratorState:
        state["prompt"] = self.build_generator_prompt(
            card_name=state["card_name"],
            user_question=state["user_question"],
            context_chunks=state["context_chunks"],
        )
        return state

    def _generate_answer_node(self, state: GeneratorState) -> GeneratorState:
        messages = [
            SystemMessage(content="ë‹¹ì‹ ì€ ì¹´ë“œë³„ ì •ë³´ ê¸°ë°˜ ì‘ë‹µì„ ì •í™•í•˜ê²Œ ìƒì„±í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤."),
            HumanMessage(content=state["prompt"]),
        ]
        state["answer"] = self.llm(messages).content
        return state

    def _rewrite_answer_node(self, state: GeneratorState) -> GeneratorState:
        if not state.get("explain_easy", False):
            state["simplified_answer"] = ""
            return state

        prompt = f"""
ë‹¹ì‹ ì€ ì‹ ìš©ì¹´ë“œë‚˜ ì²´í¬ì¹´ë“œ ì •ë³´ë¥¼ ì‚¬ìš©ìê°€ **ì •í™•í•˜ê³  ì‰½ê²Œ ì´í•´í•  ìˆ˜ ìˆë„ë¡ ì¬ì‘ì„±**í•´ì£¼ëŠ” AIì…ë‹ˆë‹¤.

ì•„ë˜ ë¬¸ì¥ì„ ë‹¤ìŒ ê¸°ì¤€ì— ë”°ë¼ **ì¹œì ˆí•˜ê²Œ ë‹¤ì‹œ ì„¤ëª…**í•´ì£¼ì„¸ìš”:

1. **ì „ë¬¸ ìš©ì–´**(ì˜ˆ: ë¦¬ë³¼ë¹™, ìœ„ë²•ê³„ì•½í•´ì§€ê¶Œ ë“±)ëŠ” ê°„ë‹¨í•œ ì˜ˆì‹œë‚˜ ì‰¬ìš´ ë§ë¡œ í’€ì–´ì„œ ì„¤ëª…í•˜ì„¸ìš”.
2. **ë¬¸ì¥ì´ ê¸¸ê³  ë³µì¡í•œ ê²½ìš°**, **í•µì‹¬ì„ ìœ ì§€**í•˜ë©´ì„œ ë¬¸ì¥ì„ ë¶„ë¦¬í•´ **ëª…í™•í•˜ê²Œ ì •ë¦¬**í•˜ì„¸ìš”.
3. **í•„ìˆ˜ ì •ë³´**(ì˜ˆ: ê¸ˆì•¡, ì¡°ê±´, ì±…ì„, ìœ ì˜ì‚¬í•­ ë“±)ëŠ” **ì ˆëŒ€ ë¹ ëœ¨ë¦¬ì§€ ë§ê³  ë°˜ì˜**í•˜ì„¸ìš”.
4. ìš”ì•½í•˜ì§€ ë§ê³ , ë§ì„ ì§€ì–´ë‚´ì§€ ë§ê³ , ë²•ì  í‘œí˜„ì„ ì‚­ì œí•˜ì§€ ë§ê³  **ë¬¸ë§¥ ê·¸ëŒ€ë¡œ ì‰½ê²Œ í’€ì–´** ì“°ì„¸ìš”.
5. ì „ì²´ì ìœ¼ë¡œ **ê³ ê° ìƒë‹´ì›ì´ ì¹œì ˆí•˜ê²Œ ì„¤ëª…í•´ì£¼ëŠ” ë§íˆ¬**ë¡œ ë°”ê¾¸ì„¸ìš”.

[ì›ë¬¸]
{state['answer']}

[ì‰¬ìš´ ì„¤ëª…]
"""
        messages = [
            SystemMessage(content="ë‹¹ì‹ ì€ ê¸ˆìœµ ì •ë³´ë¥¼ ì‰½ê²Œ ì„¤ëª…í•´ì£¼ëŠ” AIì…ë‹ˆë‹¤."),
            HumanMessage(content=prompt),
        ]
        
        gpt4_llm = ChatOpenAI(model="gpt-4", temperature=0.3)
        state["simplified_answer"] = gpt4_llm(messages).content
        return state

    def _build_langgraph(self) -> StateGraph:
        builder = StateGraph(GeneratorState)
        builder.add_node("BuildPrompt", self._build_prompt_node)
        builder.add_node("GenerateAnswer", self._generate_answer_node)
        builder.add_node("RewriteAnswer", self._rewrite_answer_node)
        builder.set_entry_point("BuildPrompt")
        builder.add_edge("BuildPrompt", "GenerateAnswer")
        builder.add_edge("GenerateAnswer", "RewriteAnswer")
        builder.add_edge("RewriteAnswer", END)
        return builder.compile()

    # -------- ì¹´ë“œ íŒŒì¼ ì°¾ê¸° --------
    def _find_card_json_path(self, card_name: str) -> Optional[str]:
        """ì¹´ë“œ ì´ë¦„ìœ¼ë¡œ JSON íŒŒì¼ ê²½ë¡œ ì°¾ê¸° (í•˜ìœ„ í´ë” ì¬ê·€ + ë¶€ë¶„ ë§¤ì¹­)."""
        print(f"ğŸ” '{card_name}' ì¹´ë“œì˜ JSON íŒŒì¼ ê²€ìƒ‰ ì¤‘...")
        needle = _normalize_name(card_name)

        # í›„ë³´ (score, path)ë¡œ ëª¨ì•„ ê°€ì¥ ì ìˆ˜ ë†’ì€ íŒŒì¼ ì„ íƒ
        candidates: list[tuple[int, str]] = []
        total_checked = 0

        def score_match(target: str) -> int:
            """
            ì¼ì¹˜ ì ìˆ˜ (ë†’ì„ìˆ˜ë¡ ìš°ì„ )
              100: ì™„ì „ ë™ì¼
               80: ì‹œì‘ ë¶€ë¶„ ì¼ì¹˜(ë˜ëŠ” ìƒí˜¸ prefix)
               60: ë¶€ë¶„ í¬í•¨
                0: ë¶ˆì¼ì¹˜
            """
            t = _normalize_name(target)
            if t == needle:
                return 100
            if t.startswith(needle) or needle.startswith(t):
                return 80
            if needle in t:
                return 60
            return 0

        for data_dir in self.data_dirs:
            if not data_dir or not os.path.exists(data_dir):
                continue

            for root, _, files in os.walk(data_dir):
                json_files = [f for f in files if f.lower().endswith(".json")]
                total_checked += len(json_files)

                for fn in json_files:
                    full = os.path.join(root, fn)

                    # 1) íŒŒì¼ëª… ê¸°ë°˜ ì ìˆ˜
                    s1 = score_match(fn)

                    # 2) JSON ë‚´ë¶€ card_name ê¸°ë°˜ ì ìˆ˜
                    s2 = 0
                    try:
                        with open(full, "r", encoding="utf-8") as f:
                            data = json.load(f)
                        inner = data.get("card_name") or ""
                        s2 = score_match(inner)
                    except Exception:
                        pass

                    score = max(s1, s2)
                    if score >= 60:  # ë¶€ë¶„ í¬í•¨ ì´ìƒë§Œ í›„ë³´ë¡œ ì±„íƒ
                        # íŒŒì¼ëª… ì™„ì „ ë™ì¼ì´ë©´ ì†Œí­ ê°€ì¤‘ì¹˜
                        if _normalize_name(fn) == needle:
                            score += 5
                        candidates.append((score, full))

        print(f"   ğŸ” ê²€ì‚¬í•œ JSON íŒŒì¼ ìˆ˜: {total_checked}")
        if not candidates:
            print("   âŒ ë§¤ì¹­ í›„ë³´ê°€ ì—†ìŠµë‹ˆë‹¤.")
            self._show_available_cards_sample()
            return None

        candidates.sort(key=lambda x: x[0], reverse=True)
        best_score, best_path = candidates[0]
        print(f"âœ… ìµœì  ë§¤ì¹­({best_score}): {best_path}")
        return best_path


    def _show_available_cards_sample(self, max_samples: int = 5):
        found_cards = []
        for data_dir in self.data_dirs:
            if not os.path.exists(data_dir):
                continue
            for p in Path(data_dir).rglob("*.json"):
                try:
                    with open(p, "r", encoding="utf-8") as f:
                        data = json.load(f)
                    name = data.get("card_name")
                    if name:
                        cat = os.path.relpath(str(p.parent), data_dir)
                        found_cards.append((name, cat))
                        print(f"   - '{name}' (ì¹´í…Œê³ ë¦¬: {cat})")
                        if len(found_cards) >= max_samples:
                            raise StopIteration
                except StopIteration:
                    break
                except Exception:
                    continue
            if len(found_cards) >= max_samples:
                break
        if not found_cards:
            print("   âŒ ì‚¬ìš© ê°€ëŠ¥í•œ ì¹´ë“œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        else:
            print(f"   ğŸ’¡ ì´ {len(found_cards)}ê°œ ìƒ˜í”Œ í‘œì‹œ (ì‹¤ì œë¡œëŠ” ë” ë§ì„ ìˆ˜ ìˆìŒ)")

    # -------- ì¹´ë“œë³„ ì¤€ë¹„/ê²€ìƒ‰ --------
    def _prepare_individual_card_data(self, card_name: str, json_path: str):
        documents = self.load_documents_field_level([json_path])
        if not documents:
            raise ValueError(f"'{card_name}' ì¹´ë“œì˜ ë¬¸ì„œê°€ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤.")

        faiss_index = FAISS.from_documents(documents, self.embedding_model)
        bm25 = BM25Retriever.from_documents(documents)
        bm25.k = 60
        return documents, faiss_index, bm25

    def _prepare_card_data(self, card_name: str):
        print(f"ğŸ”§ '{card_name}' ì¹´ë“œ ë°ì´í„° ì¤€ë¹„ ì¤‘...")

        if card_name in self._document_cache:
            print("ğŸ’¾ ê°œë³„ ì¹´ë“œ ìºì‹œ ì‚¬ìš© ì¤‘...")
            return (
                self._document_cache[card_name],
                self._faiss_cache[card_name],
                self._bm25_cache[card_name],
            )

        json_path = self._find_card_json_path(card_name)
        if not json_path:
            raise ValueError(f"'{card_name}' ì¹´ë“œì˜ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

        category = self._get_card_category_from_path(json_path)
        cat_docs, cat_faiss, cat_bm25 = self._load_category_embeddings(category)

        if cat_docs is None:
            print(f"âš ï¸ {category.upper()} ì¹´í…Œê³ ë¦¬ ì„ë² ë”©ì´ ì—†ì–´ ê°œë³„ ì²˜ë¦¬í•©ë‹ˆë‹¤...")
            docs, fidx, bidx = self._prepare_individual_card_data(card_name, json_path)
        else:
            needle = _normalize_name(card_name)
            card_docs = [d for d in cat_docs if _normalize_name(d.metadata.get("card_name","")) == needle]
            if not card_docs:
                raise ValueError(f"ì¹´í…Œê³ ë¦¬ ì„ë² ë”©ì— '{card_name}' ì¹´ë“œê°€ ì—†ìŠµë‹ˆë‹¤.")
            fidx = FAISS.from_documents(card_docs, self.embedding_model)
            bidx = BM25Retriever.from_documents(card_docs); bidx.k = 60
            docs = card_docs

        self._document_cache[card_name] = docs
        self._faiss_cache[card_name] = fidx
        self._bm25_cache[card_name] = bidx
        return docs, fidx, bidx

    # -------- ì§ˆì˜ --------
    def query(self, card_name: str, card_text: str, question: str, explain_easy: bool = False, top_k: int = 20) -> str:
        print("\n" + "=" * 60)
        print("ğŸš€ ì§ˆì˜ì‘ë‹µ ì‹œì‘")
        print(f"   - ì¹´ë“œëª…: {card_name}")
        print(f"   - ì§ˆë¬¸: {question}")
        print(f"   - ì‰¬ìš´ ì„¤ëª…: {explain_easy}")
        print(f"   - Top-K: {top_k}")
        print("=" * 60)

        try:
            documents, faiss_index, bm25 = self._prepare_card_data(card_name)

            print("ğŸ“Š ë¬¸ì„œ ê²€ìƒ‰ ë° ë­í‚¹ ì¤‘...")
            faiss_results = faiss_index.similarity_search(question, k=60)
            bm25_results = bm25.get_relevant_documents(question)

            rrf_candidates = self.reciprocal_rank_fusion(faiss_results, bm25_results)
            print(f"ğŸ”„ Cross-Encoder ì¬ë­í‚¹ ì‹¤í–‰... (ì´ {len(rrf_candidates)}ê°œ í›„ë³´)")
            inputs = [(question, chunk) for chunk in rrf_candidates]
            scores = self.reranker.predict(inputs)
            reranked_chunks = [x for _, x in sorted(zip(scores, rrf_candidates), reverse=True)]
            top_chunks = reranked_chunks[:top_k]
            print(f"âœ… ì¬ë­í‚¹ ì™„ë£Œ - ìµœì¢… {len(top_chunks)}ê°œ ì²­í¬ ì„ íƒ")

            initial_state: GeneratorState = {
                "card_name": card_name,
                "user_question": question,
                "context_chunks": top_chunks,
                "prompt": "",
                "answer": "",
                "simplified_answer": "",
                "explain_easy": explain_easy,
            }
            result = self.graph.invoke(initial_state)

            final_answer = result["simplified_answer"] if (explain_easy and result["simplified_answer"]) else result["answer"]
            print(f"ğŸ‰ ì§ˆì˜ì‘ë‹µ ì™„ë£Œ! (ìµœì¢… ë‹µë³€ ê¸¸ì´: {len(final_answer)}ì)")
            return final_answer
        except Exception as e:
            msg = f"âŒ '{card_name}' ì¹´ë“œ ì§ˆì˜ì‘ë‹µ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}"
            print(msg)
            return msg

    # -------- ê¸°íƒ€ --------
    def clear_cache(self):
        print("ğŸ—‘ï¸ ìºì‹œ í´ë¦¬ì–´ ì¤‘...")
        self._document_cache.clear()
        self._faiss_cache.clear()
        self._bm25_cache.clear()
        print("âœ… ìºì‹œê°€ í´ë¦¬ì–´ë˜ì—ˆìŠµë‹ˆë‹¤.")

    def list_available_embeddings(self):
        print(f"ğŸ“ ì„ë² ë”© ë””ë ‰í† ë¦¬: {self.embeddings_dir}")
        if not os.path.exists(self.embeddings_dir):
            print("âŒ ì„ë² ë”© ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
            return
        files = os.listdir(self.embeddings_dir)
        if not files:
            print("ğŸ“‚ ì„ë² ë”© íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
            return
        print("ğŸ“‹ ë°œê²¬ëœ ì„ë² ë”© íŒŒì¼:")
        for file in sorted(files):
            path = os.path.join(self.embeddings_dir, file)
            size = os.path.getsize(path) / (1024 * 1024)
            print(f"   - {file} ({size:.2f} MB)")


if __name__ == "__main__":
    print("ğŸš€ FAISSRAGRetriever í…ŒìŠ¤íŠ¸ ì‹œì‘")
    rag = FAISSRAGRetriever()
    rag.list_available_embeddings()

    build_embeddings = input("\nğŸ’¡ ì¹´í…Œê³ ë¦¬ë³„ ì„ë² ë”©ì„ ë¹Œë“œí•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/N): ").lower().strip()
    if build_embeddings in ["y", "yes"]:
        force_rebuild = input("ğŸ”„ ê¸°ì¡´ ì„ë² ë”©ì„ ê°•ì œë¡œ ì¬ë¹Œë“œí•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/N): ").lower().strip() in ["y", "yes"]
        rag.build_category_embeddings(force_rebuild=force_rebuild)

    latest_card = rag.get_latest_card_from_selected_cards()
    test_card = latest_card or "K-íŒ¨ìŠ¤ì¹´ë“œ"
    if not latest_card:
        print(f"âš ï¸ selected_cards.jsonì—ì„œ ì¹´ë“œë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ì–´ ê¸°ë³¸ê°’ ì‚¬ìš©: {test_card}")

    q = "ì´ ì¹´ë“œì˜ ì—°íšŒë¹„ëŠ” ì–¼ë§ˆì¸ê°€ìš”?"
    try:
        print("\nğŸ” ê¸°ë³¸ ë‹µë³€ í…ŒìŠ¤íŠ¸")
        print(rag.query(card_name=test_card, card_text="", question=q))

        print("\nğŸ” ì‰¬ìš´ ì„¤ëª… í…ŒìŠ¤íŠ¸")
        print(rag.query(card_name=test_card, card_text="", question=q, explain_easy=True))
    except Exception as e:
        print(f"âŒ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
