import os
import re
import json
import hashlib
import pickle
from tqdm import tqdm
from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.retrievers import BM25Retriever
from langchain.docstore.document import Document
from collections import defaultdict
from sentence_transformers import CrossEncoder
from langgraph.graph import StateGraph, END
from typing import TypedDict, Optional
from langchain_openai import ChatOpenAI
from langchain.schema import SystemMessage, HumanMessage
from dotenv import load_dotenv
import numpy as np


class GeneratorState(TypedDict):
    card_name: str
    user_question: str
    context_chunks: list[str]
    prompt: str
    answer: str
    simplified_answer: str
    explain_easy: bool


class FAISSRAGRetriever:
    """Original RAG ì‹œìŠ¤í…œ - ì¹´ë“œë³„ ìƒì„¸ ì •ë³´ ê²€ìƒ‰ ë° ì§ˆì˜ì‘ë‹µ"""
    
    def __init__(self):
        """Original RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™”"""
        os.environ['KMP_DUPLICATE_LIB_OK'] = 'TRUE'
        load_dotenv()
        
        self.current_file_dir = os.path.dirname(os.path.abspath(__file__))
        self.base_path = os.path.dirname(self.current_file_dir)  # í”„ë¡œì íŠ¸ ë£¨íŠ¸ í´ë”
        self.data_dirs = [
            os.path.join(self.base_path, "JSON", "ì‹ ìš©json"),
            os.path.join(self.base_path, "JSON", "ì²´í¬json")
        ]

        self.selected_cards_path = os.path.join(self.current_file_dir, "selected_cards.json")
        
        # ì„ë² ë”© ì €ì¥ ë””ë ‰í† ë¦¬ ìƒì„±
        self.embeddings_dir = os.path.join(self.current_file_dir, "original_embeddings")
        os.makedirs(self.embeddings_dir, exist_ok=True)
        print(f"ğŸ“ ì„ë² ë”© ì €ì¥ ë””ë ‰í† ë¦¬: {self.embeddings_dir}")
        
        self.llm = ChatOpenAI(model="gpt-4o", temperature=0.3)
        
        self.embedding_model = HuggingFaceEmbeddings(
            model_name="BAAI/bge-m3", 
            model_kwargs={"device": "cpu"}
        )
        
        self.reranker = CrossEncoder("cross-encoder/ms-marco-MiniLM-L-6-v2")
        
        self.graph = self._build_langgraph()
        
        # ì¹´ë“œë³„ ìºì‹œëœ ë¬¸ì„œì™€ ì¸ë±ìŠ¤ ì €ì¥
        self._document_cache = {}
        self._faiss_cache = {}
        self._bm25_cache = {}
        
        print("ğŸ‰ FAISSRAGRetriever ì´ˆê¸°í™” ì™„ë£Œ!")

    def get_latest_card_from_selected_cards(self) -> Optional[str]:
        """selected_cards.jsonì—ì„œ ë§ˆì§€ë§‰ ì¹´ë“œ ì´ë¦„ ê°€ì ¸ì˜¤ê¸°"""
        
        try:
            if not os.path.exists(self.selected_cards_path):
                print(f"âŒ selected_cards.json íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {self.selected_cards_path}")
                return None
            
            with open(self.selected_cards_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            if not data:
                print("âŒ selected_cards.jsonì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤")
                return None
            
            # ë§ˆì§€ë§‰ í•­ëª© ê°€ì ¸ì˜¤ê¸°
            last_card = data[-1]
            card_name = last_card.get("card_name")
            
            print(f"ğŸ¯ ë§ˆì§€ë§‰ ì¹´ë“œ ì •ë³´:")
            print(f"   - Card Name: {card_name}")
            print(f"   - Timestamp: {last_card.get('timestamp', 'N/A')}")
            print(f"   - Question: {last_card.get('question', 'N/A')}")
            print(f"   - Card Type: {last_card.get('card_type', 'N/A')}")
            print(f"   - Keyword: {last_card.get('keyword', 'N/A')}")
            
            return card_name
            
        except Exception as e:
            print(f"âŒ selected_cards.json ì½ê¸° ì¤‘ ì˜¤ë¥˜: {e}")
            return None

    def load_documents_field_level(self, json_paths: list[str]) -> list[Document]:
        """Field ë‹¨ìœ„ë¡œ ë¬¸ì„œ ë¡œë”©"""
        print(f"ğŸ“‹ ë¬¸ì„œ ë¡œë”© ì‹œì‘... (ì´ {len(json_paths)}ê°œ íŒŒì¼)")
        documents = []
        
        for json_path in tqdm(json_paths, desc="ğŸ“‹ ë¬¸ì„œ ë¡œë”© ì¤‘"):
            try:
                print(f"ğŸ“„ íŒŒì¼ ì²˜ë¦¬ ì¤‘: {json_path}")
                with open(json_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                card_name = data.get("card_name", "UnknownCard")
                print(f"   - ì¹´ë“œëª…: {card_name}")
                
                sections_count = 0
                for section in data.get("sections", []):
                    heading = section.get("heading", "")
                    subheading = section.get("subheading", "")
                    
                    # ë‹¤ì–‘í•œ í•„ë“œì—ì„œ ë‚´ìš© ì¶”ì¶œ
                    for key in ['benefit', 'benefits', 'fee', 'agreement', 'condition', 'conditions', 'etc', 'overseas_usage']:
                        val = section.get(key)
                        if not val:
                            continue
                            
                        contents = val if isinstance(val, list) else [val]
                        merged_text = "\n".join([c.strip() for c in contents if len(c.strip()) > 0])
                        
                        if len(merged_text.strip()) < 10:
                            continue
                            
                        full_text = f"[{card_name}]\n{heading} - {subheading}\n<{key}>\n{merged_text}"
                        documents.append(Document(
                            page_content=full_text,
                            metadata={
                                "card_name": card_name, 
                                "field": key, 
                                "heading": heading, 
                                "subheading": subheading
                            }
                        ))
                        sections_count += 1      
                        
            except Exception as e:
                print(f"â— Error reading {json_path}: {e}")

        return documents

    def _get_card_category(self, json_path: str) -> str:
        """ì¹´ë“œ JSON íŒŒì¼ ê²½ë¡œì—ì„œ ì¹´í…Œê³ ë¦¬(ì‹ ìš©/ì²´í¬) íŒë³„"""
        if "ì‹ ìš©json" in json_path:
            return "credit"
        elif "ì²´í¬json" in json_path:
            return "check"
        else:
            return "unknown"

    def _load_category_documents(self, category: str) -> list[Document]:
        """ì¹´í…Œê³ ë¦¬ë³„ë¡œ ëª¨ë“  ë¬¸ì„œ ë¡œë”© (ì‹ ìš©ì¹´ë“œ or ì²´í¬ì¹´ë“œ)"""
        print(f"ğŸ“‹ {category.upper()} ì¹´ë“œ ì¹´í…Œê³ ë¦¬ ë¬¸ì„œ ë¡œë”© ì¤‘...")
        
        target_dir = None
        if category == "credit":
            target_dir = os.path.join(self.base_path, "JSON", "ì‹ ìš©json")
        elif category == "check":
            target_dir = os.path.join(self.base_path, "JSON", "ì²´í¬json")
        else:
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ì¹´í…Œê³ ë¦¬: {category}")
        
        if not os.path.exists(target_dir):
            print(f"âŒ ì¹´í…Œê³ ë¦¬ ë””ë ‰í† ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {target_dir}")
            return []
        
        # ëª¨ë“  JSON íŒŒì¼ ê²½ë¡œ ìˆ˜ì§‘
        json_files = []
        for root, dirs, files in os.walk(target_dir):
            for file in files:
                if file.endswith('.json'):
                    json_files.append(os.path.join(root, file))
        
        print(f"ğŸ“Š {category.upper()} ì¹´í…Œê³ ë¦¬ì—ì„œ {len(json_files)}ê°œ JSON íŒŒì¼ ë°œê²¬")
        
        if not json_files:
            print(f"âš ï¸ {category.upper()} ì¹´í…Œê³ ë¦¬ì— JSON íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤")
            return []
        
        return self.load_documents_field_level(json_files)

    def _save_category_embeddings(self, category: str, documents: list[Document], faiss_index: FAISS) -> bool:
        """ì¹´í…Œê³ ë¦¬ë³„ ì„ë² ë”© ë°ì´í„° ì €ì¥"""
        print(f"ğŸ’¾ {category.upper()} ì¹´ë“œ ì„ë² ë”© ì €ì¥ ì¤‘...")
        
        try:
            # íŒŒì¼ ê²½ë¡œ ì •ì˜
            base_filename = f"{category}_card"
            faiss_path = os.path.join(self.embeddings_dir, f"{base_filename}_embeddings.faiss")
            pkl_path = os.path.join(self.embeddings_dir, f"{base_filename}_embedding_data.pkl")
            metadata_path = os.path.join(self.embeddings_dir, f"{base_filename}_metadata.json")
            texts_path = os.path.join(self.embeddings_dir, f"{base_filename}_texts.txt")
            
            print(f"   ğŸ“ ì €ì¥ ê²½ë¡œ:")
            print(f"      - FAISS ì¸ë±ìŠ¤: {faiss_path}")
            print(f"      - ì„ë² ë”© ë°ì´í„°: {pkl_path}")
            print(f"      - ë©”íƒ€ë°ì´í„°: {metadata_path}")
            print(f"      - í…ìŠ¤íŠ¸: {texts_path}")
            
            # 1. FAISS ì¸ë±ìŠ¤ë§Œ ì €ì¥ (langchainì˜ ë³µì¡í•œ êµ¬ì¡° ëŒ€ì‹ )
            if hasattr(faiss_index, 'index'):
                import faiss as faiss_lib
                faiss_lib.write_index(faiss_index.index, faiss_path)
            
            # 2. ë¬¸ì„œ ë°ì´í„°ë¥¼ pickleë¡œ ì €ì¥
            embedding_data = {
                'documents': documents,
                'texts': [doc.page_content for doc in documents],
                'metadatas': [doc.metadata for doc in documents],
                'embeddings': faiss_index.index.reconstruct_n(0, faiss_index.index.ntotal) if hasattr(faiss_index.index, 'reconstruct_n') else None
            }
            
            with open(pkl_path, 'wb') as f:
                pickle.dump(embedding_data, f)
            print("   âœ… ì„ë² ë”© ë°ì´í„° ì €ì¥ ì™„ë£Œ")
            
            # 3. ë©”íƒ€ë°ì´í„°ë¥¼ JSONìœ¼ë¡œ ì €ì¥
            import datetime
            metadata_summary = {
                'category': category,
                'total_documents': len(documents),
                'cards': list(set([doc.metadata.get('card_name', 'Unknown') for doc in documents])),
                'created_at': str(datetime.datetime.now().isoformat()),
                'embedding_model': 'BAAI/bge-m3'
            }
            
            with open(metadata_path, 'w', encoding='utf-8') as f:
                json.dump(metadata_summary, f, ensure_ascii=False, indent=2)
            print("   âœ… ë©”íƒ€ë°ì´í„° ì €ì¥ ì™„ë£Œ")
            
            # 4. í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ txtë¡œ ì €ì¥ (ê²€ìˆ˜ìš©)
            with open(texts_path, 'w', encoding='utf-8') as f:
                f.write(f"=== {category.upper()} ì¹´ë“œ ë¬¸ì„œ í…ìŠ¤íŠ¸ ===\n")
                f.write(f"ì´ ë¬¸ì„œ ìˆ˜: {len(documents)}\n")
                f.write(f"í¬í•¨ëœ ì¹´ë“œ: {', '.join(metadata_summary['cards'])}\n")
                f.write("="*50 + "\n\n")
                
                for i, doc in enumerate(documents):
                    f.write(f"ë¬¸ì„œ {i+1}:\n")
                    f.write(f"ì¹´ë“œëª…: {doc.metadata.get('card_name', 'Unknown')}\n")
                    f.write(f"í•„ë“œ: {doc.metadata.get('field', 'Unknown')}\n")
                    f.write(f"ì œëª©: {doc.metadata.get('heading', '')} - {doc.metadata.get('subheading', '')}\n")
                    f.write("-"*30 + "\n")
                    f.write(doc.page_content[:500] + "...\n" if len(doc.page_content) > 500 else doc.page_content + "\n")
                    f.write("\n" + "="*50 + "\n\n")
            
            print(f"âœ… {category.upper()} ì¹´ë“œ ì„ë² ë”© ì €ì¥ ì„±ê³µ!")
            return True
            
        except Exception as e:
            print(f"âŒ {category.upper()} ì¹´ë“œ ì„ë² ë”© ì €ì¥ ì‹¤íŒ¨: {e}")
            import traceback
            traceback.print_exc()
            return False

    def _load_category_embeddings(self, category: str) -> tuple[list[Document], FAISS, BM25Retriever]:
        """ì¹´í…Œê³ ë¦¬ë³„ ì„ë² ë”© ë¡œë“œ"""
        
        base_filename = f"{category}_card"
        faiss_path = os.path.join(self.embeddings_dir, f"{base_filename}_embeddings.faiss")
        pkl_path = os.path.join(self.embeddings_dir, f"{base_filename}_embedding_data.pkl")
        
        # íŒŒì¼ ì¡´ì¬ í™•ì¸
        if not os.path.exists(pkl_path):
            print(f"âš ï¸ {category.upper()} ì¹´ë“œ ì„ë² ë”© íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {pkl_path}")
            return None, None, None
        
        try:
            # ë¬¸ì„œ ë°ì´í„° ë¡œë“œ
            with open(pkl_path, 'rb') as f:
                embedding_data = pickle.load(f)
            
            documents = embedding_data['documents']
            
            # FAISS ì¸ë±ìŠ¤ ì¬êµ¬ì„±
            if os.path.exists(faiss_path):
                # FAISS íŒŒì¼ì´ ìˆë‹¤ë©´ ë¡œë“œ
                import faiss as faiss_lib
                index = faiss_lib.read_index(faiss_path)
                
                # Langchain FAISS ê°ì²´ ì¬êµ¬ì„±
                faiss_index = FAISS(
                    embedding_function=self.embedding_model,
                    index=index,
                    docstore={i: doc for i, doc in enumerate(documents)},
                    index_to_docstore_id={i: i for i in range(len(documents))}
                )
                print(f"   ğŸ” FAISS ì¸ë±ìŠ¤ ë¡œë“œ ì™„ë£Œ")
            else:
                # FAISS íŒŒì¼ì´ ì—†ë‹¤ë©´ ë‹¤ì‹œ ìƒì„±
                print(f"   âš ï¸ FAISS íŒŒì¼ì´ ì—†ì–´ì„œ ë‹¤ì‹œ ìƒì„±í•©ë‹ˆë‹¤...")
                faiss_index = FAISS.from_documents(documents, self.embedding_model)
            
            # BM25 ì¸ë±ìŠ¤ ìƒì„±
            bm25 = BM25Retriever.from_documents(documents)
            bm25.k = 60
            
            print(f"âœ… {category.upper()} ì¹´ë“œ ì„ë² ë”© ë¡œë“œ ì„±ê³µ!")
            return documents, faiss_index, bm25
            
        except Exception as e:
            print(f"âŒ {category.upper()} ì¹´ë“œ ì„ë² ë”© ë¡œë“œ ì‹¤íŒ¨: {e}")
            import traceback
            traceback.print_exc()
            return None, None, None

    def build_category_embeddings(self, force_rebuild: bool = False):
        """ì‹ ìš©ì¹´ë“œ/ì²´í¬ì¹´ë“œ ì¹´í…Œê³ ë¦¬ë³„ ì„ë² ë”© ë¹Œë“œ"""
        
        for category in ["credit", "check"]:
            print(f"\n{'='*50}")
            print(f"{'='*50}")
            
            base_filename = f"{category}_card"
            pkl_path = os.path.join(self.embeddings_dir, f"{base_filename}_embedding_data.pkl")
            
            # ì´ë¯¸ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
            if os.path.exists(pkl_path) and not force_rebuild:
                print(f"âœ… {category.upper()} ì¹´ë“œ ì„ë² ë”©ì´ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤: {pkl_path}")
                print("   force_rebuild=Trueë¡œ ì„¤ì •í•˜ë©´ ì¬ë¹Œë“œë©ë‹ˆë‹¤.")
                continue
            
            # ì¹´í…Œê³ ë¦¬ë³„ ë¬¸ì„œ ë¡œë”©
            documents = self._load_category_documents(category)
            
            if not documents:
                print(f"âš ï¸ {category.upper()} ì¹´í…Œê³ ë¦¬ì— ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤. ìŠ¤í‚µí•©ë‹ˆë‹¤.")
                continue
            
            print(f"ğŸ“Š {category.upper()} ì¹´í…Œê³ ë¦¬ í†µê³„:")
            cards = list(set([doc.metadata.get('card_name', 'Unknown') for doc in documents]))
            print(f"   - ì´ ë¬¸ì„œ ìˆ˜: {len(documents)}")
            print(f"   - ì¹´ë“œ ìˆ˜: {len(cards)}")
            print(f"   - í¬í•¨ëœ ì¹´ë“œ: {', '.join(cards[:5])}{'...' if len(cards) > 5 else ''}")
            
            # FAISS ì¸ë±ìŠ¤ ìƒì„±
            print(f"ğŸ” {category.upper()} ì¹´ë“œ FAISS ì¸ë±ìŠ¤ ìƒì„± ì¤‘...")
            faiss_index = FAISS.from_documents(documents, self.embedding_model)
            print(f"âœ… {category.upper()} ì¹´ë“œ FAISS ì¸ë±ìŠ¤ ìƒì„± ì™„ë£Œ")
            
            # ì„ë² ë”© ì €ì¥
            success = self._save_category_embeddings(category, documents, faiss_index)
            
            if success:
                print(f"ğŸ‰ {category.upper()} ì¹´ë“œ ì„ë² ë”© ë¹Œë“œ ì™„ë£Œ!")
            else:
                print(f"âŒ {category.upper()} ì¹´ë“œ ì„ë² ë”© ë¹Œë“œ ì‹¤íŒ¨!")
        
        print(f"ğŸ“ ì €ì¥ ìœ„ì¹˜: {self.embeddings_dir}")

    def reciprocal_rank_fusion(self, faiss_results: list, bm25_results: list, k: int = 60) -> list[str]:
        """RRF Score ê³„ì‚°ìœ¼ë¡œ ê²°ê³¼ ìœµí•©"""
        scores = defaultdict(float)
        
        def update_scores(results, weight):
            for rank, item in enumerate(results):
                key = item.page_content.strip()
                scores[key] += weight / (k + rank + 1)
        
        update_scores(faiss_results, weight=0.6)
        update_scores(bm25_results, weight=0.4)
        
        sorted_chunks = sorted(scores.items(), key=lambda x: x[1], reverse=True)
        result_chunks = [chunk for chunk, _ in sorted_chunks[:k]]
        return result_chunks

    def build_generator_prompt(self, card_name: str, user_question: str, context_chunks: list[str]) -> str:
        """ë‹µë³€ ìƒì„±ìš© í”„ë¡¬í”„íŠ¸ êµ¬ì„±"""
        context_text = "\n".join(context_chunks)
        return f"""
ë‹¹ì‹ ì€ ì‹ ìš©ì¹´ë“œ ë° ì²´í¬ì¹´ë“œ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ì ì§ˆë¬¸ì— ëŒ€í•´ **ë¬¸ì„œ ê¸°ë°˜ìœ¼ë¡œ êµ¬ì²´ì ì´ê³  ì •í™•í•œ ë‹µë³€ì„ ì œê³µí•˜ëŠ” AI ì „ë¬¸ê°€**ì…ë‹ˆë‹¤.

ì¹´ë“œ ì´ë¦„: {card_name}

ì•„ë˜ëŠ” í•´ë‹¹ ì¹´ë“œì˜ ì•½ê´€ ë° ìƒí’ˆì„¤ëª…ì„œì—ì„œ ì¶”ì¶œëœ ì¼ë¶€ ë¬¸ì„œ ë‚´ìš©ì…ë‹ˆë‹¤:

[ë¬¸ì„œ ë‚´ìš©]
{context_text}

[ì‚¬ìš©ì ì§ˆë¬¸]
{user_question}

[ì‘ë‹µ ì¡°ê±´]
1. ë°˜ë“œì‹œ **ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ë§ê²Œ ìœ„ ë¬¸ì„œ ë‚´ìš©ì— ê¸°ë°˜**í•˜ì—¬ ë‹µë³€í•˜ê³ , ê°€ëŠ¥í•œ í•œ **êµ¬ì²´ì ì´ê³  í’ë¶€í•œ ì„¤ëª…**ì„ ì œê³µí•˜ì„¸ìš”.
   (ì˜ˆ: í˜œíƒ ì—…ì¢…, í˜œíƒ ì¡°ê±´, ë¶€ê°€ í˜œíƒ, í•´ì§€ ì¡°ê±´ ë“±)
2. ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ë‹µë³€í•  ë•Œ **ì •í™•í•œ ìˆ˜ì¹˜, ì‹œê¸°, ì¡°ê±´, ë¬¸êµ¬**ë¥¼ ë°˜ì˜í•˜ì„¸ìš”.
   (ì˜ˆ: "ì›” ìµœëŒ€ 2íšŒ", "êµ­ë‚´ì „ìš© 9,000ì›", "ì „ì›” 30ë§Œì› ì´ìƒ ì´ìš© ì‹œ ì ìš©")
3. **ì¹´ë“œ ì•½ê´€ì˜ ì£¼ì˜ì‚¬í•­ì´ë‚˜ í™•ì¸ì‚¬í•­** (ì˜ˆ: ì „ì›” ì‹¤ì  ì œì™¸ ì¡°ê±´, ì†Œë¹„ì ê¶Œë¦¬, ì—°ì²´ ì‹œ ë¶ˆì´ìµ ë“±)ë„ ë¬¸ë§¥ì— ë”°ë¼ í¬í•¨í•´ ì£¼ì„¸ìš”.
4. ì‚¬ìš©ìì˜ ì§ˆë¬¸ì´ ì—¬ëŸ¬ í•­ëª©ì„ í¬í•¨í•  ê²½ìš°, ê° í•­ëª©ë³„ë¡œ **ì²´ê³„ì ìœ¼ë¡œ êµ¬ì„±í•˜ì—¬ ë‹µë³€**í•˜ì„¸ìš”.
5. ë¬¸ì„œì— ëª…ì‹œë˜ì§€ ì•Šì€ ë‚´ìš©ì€ "ë¬¸ì„œì— ëª…ì‹œëœ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤."ë¼ê³  ë‹µë³€í•˜ê³ , **ì ˆëŒ€ ê±°ì§“ ì •ë³´ë¥¼ ì§€ì–´ë‚´ì„œëŠ” ì•ˆë©ë‹ˆë‹¤.**
""".strip()

    # LangGraph ë…¸ë“œ ì •ì˜
    def _build_prompt_node(self, state: GeneratorState) -> GeneratorState:
        print("ğŸ”§ í”„ë¡¬í”„íŠ¸ ë…¸ë“œ ì‹¤í–‰ ì¤‘...")
        state["prompt"] = self.build_generator_prompt(
            card_name=state["card_name"],
            user_question=state["user_question"],
            context_chunks=state["context_chunks"]
        )
        return state

    def _generate_answer_node(self, state: GeneratorState) -> GeneratorState:
        messages = [
            SystemMessage(content="ë‹¹ì‹ ì€ ì¹´ë“œë³„ ì •ë³´ ê¸°ë°˜ ì‘ë‹µì„ ì •í™•í•˜ê²Œ ìƒì„±í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤."),
            HumanMessage(content=state["prompt"])
        ]
        state["answer"] = self.llm(messages).content
        return state

    def _rewrite_answer_node(self, state: GeneratorState) -> GeneratorState:
        
        if not state.get("explain_easy", False):
            state["simplified_answer"] = ""
            return state

        prompt = f"""
ë‹¹ì‹ ì€ ì‹ ìš©ì¹´ë“œë‚˜ ì²´í¬ì¹´ë“œ ì •ë³´ë¥¼ ì‚¬ìš©ìê°€ **ì •í™•í•˜ê³  ì‰½ê²Œ ì´í•´í•  ìˆ˜ ìˆë„ë¡ ì¬ì‘ì„±**í•´ì£¼ëŠ” AIì…ë‹ˆë‹¤.

ì•„ë˜ ë¬¸ì¥ì„ ë‹¤ìŒ ê¸°ì¤€ì— ë”°ë¼ **ì¹œì ˆí•˜ê²Œ ë‹¤ì‹œ ì„¤ëª…**í•´ì£¼ì„¸ìš”:

1. **ì „ë¬¸ ìš©ì–´**(ì˜ˆ: ë¦¬ë³¼ë¹™, ìœ„ë²•ê³„ì•½í•´ì§€ê¶Œ ë“±)ëŠ” ê°„ë‹¨í•œ ì˜ˆì‹œë‚˜ ì‰¬ìš´ ë§ë¡œ í’€ì–´ì„œ ì„¤ëª…í•˜ì„¸ìš”.
2. **ë¬¸ì¥ì´ ê¸¸ê³  ë³µì¡í•œ ê²½ìš°**, **í•µì‹¬ì„ ìœ ì§€**í•˜ë©´ì„œ ë¬¸ì¥ì„ ë¶„ë¦¬í•´ **ëª…í™•í•˜ê²Œ ì •ë¦¬**í•˜ì„¸ìš”.
3. **í•„ìˆ˜ ì •ë³´**(ì˜ˆ: ê¸ˆì•¡, ì¡°ê±´, ì±…ì„, ìœ ì˜ì‚¬í•­ ë“±)ëŠ” **ì ˆëŒ€ ë¹ ëœ¨ë¦¬ì§€ ë§ê³  ë°˜ì˜**í•˜ì„¸ìš”.
4. ë„ˆë¬´ ë‹¨ìˆœí™”í•˜ê±°ë‚˜, ë§ì„ ì§€ì–´ë‚´ê±°ë‚˜, ë²•ì  í‘œí˜„ì„ ì‚­ì œí•˜ì§€ ë§ê³  **ë¬¸ë§¥ ê·¸ëŒ€ë¡œ ì‰½ê²Œ í’€ì–´** ì“°ì„¸ìš”.
5. ì „ì²´ì ìœ¼ë¡œ **ê³ ê° ìƒë‹´ì›ì´ ì¹œì ˆí•˜ê²Œ ì„¤ëª…í•´ì£¼ëŠ” ë§íˆ¬**ë¡œ ë°”ê¾¸ì„¸ìš”.

[ì›ë¬¸]
{state['answer']}

[ì‰¬ìš´ ì„¤ëª…]
"""
        messages = [
            SystemMessage(content="ë‹¹ì‹ ì€ ê¸ˆìœµ ì •ë³´ë¥¼ ì‰½ê²Œ ì„¤ëª…í•´ì£¼ëŠ” AIì…ë‹ˆë‹¤."),
            HumanMessage(content=prompt)
        ]
        state["simplified_answer"] = self.llm(messages).content
        print(f"âœ… ì‰¬ìš´ ì„¤ëª… ìƒì„± ì™„ë£Œ (ê¸¸ì´: {len(state['simplified_answer'])}ì)")
        return state

    def _build_langgraph(self) -> StateGraph:
        """LangGraph êµ¬ì„±"""
        print("ğŸ”§ LangGraph êµ¬ì„± ì¤‘...")
        builder = StateGraph(GeneratorState)
        builder.add_node("BuildPrompt", self._build_prompt_node)
        builder.add_node("GenerateAnswer", self._generate_answer_node)
        builder.add_node("RewriteAnswer", self._rewrite_answer_node)
        builder.set_entry_point("BuildPrompt")
        builder.add_edge("BuildPrompt", "GenerateAnswer")
        builder.add_edge("GenerateAnswer", "RewriteAnswer")
        builder.add_edge("RewriteAnswer", END)
        print("âœ… LangGraph êµ¬ì„± ì™„ë£Œ")
        return builder.compile()

    def _find_card_json_path(self, card_name: str) -> Optional[str]:
        """ì¹´ë“œ ì´ë¦„ìœ¼ë¡œ JSON íŒŒì¼ ê²½ë¡œ ì°¾ê¸° (ì¹´í…Œê³ ë¦¬ í´ë” í¬í•¨ ì¬ê·€ ê²€ìƒ‰)"""
        print(f"ğŸ” '{card_name}' ì¹´ë“œì˜ JSON íŒŒì¼ ê²€ìƒ‰ ì¤‘...")
        
        for data_dir in self.data_dirs:
            print(f"ğŸ“‚ ë©”ì¸ ë””ë ‰í† ë¦¬ ê²€ìƒ‰: {data_dir}")
            if not os.path.exists(data_dir):
                print(f"âš ï¸ ë””ë ‰í† ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {data_dir}")
                continue
            
            # ë””ë ‰í† ë¦¬ ë‚´ìš© í™•ì¸
            try:
                items = os.listdir(data_dir)
                print(f"   ğŸ“‹ ë””ë ‰í† ë¦¬ ë‚´ìš©: {items}")
            except Exception as e:
                print(f"   â— ë””ë ‰í† ë¦¬ ì½ê¸° ì˜¤ë¥˜: {e}")
                continue
            
            # ì¬ê·€ì ìœ¼ë¡œ ëª¨ë“  í•˜ìœ„ ë””ë ‰í† ë¦¬ì™€ íŒŒì¼ ê²€ìƒ‰
            total_files_checked = 0
            categories_found = []
            
            for root, dirs, files in os.walk(data_dir):
                # í˜„ì¬ ê²€ìƒ‰ ì¤‘ì¸ ì¹´í…Œê³ ë¦¬ í´ë” í‘œì‹œ
                relative_path = os.path.relpath(root, data_dir)
                if relative_path != ".":
                    categories_found.append(relative_path)
                
                json_files = [f for f in files if f.endswith('.json')]
                total_files_checked += len(json_files)
                
                if json_files:
                    print(f"      - JSON íŒŒì¼ {len(json_files)}ê°œ ë°œê²¬: {json_files[:3]}{'...' if len(json_files) > 3 else ''}")
                
                for filename in json_files:
                    try:
                        filepath = os.path.join(root, filename)
                        with open(filepath, 'r', encoding='utf-8') as f:
                            data = json.load(f)
                            file_card_name = data.get("card_name")
                            print(f"            - íŒŒì¼ ë‚´ ì¹´ë“œëª…: '{file_card_name}'")
                            
                            if file_card_name == card_name:
                                print(f"âœ… ë§¤ì¹­ë˜ëŠ” íŒŒì¼ ë°œê²¬!")
                                print(f"   - íŒŒì¼ ê²½ë¡œ: {filepath}")
                                print(f"   - íŒŒì¼ì˜ ì¹´ë“œëª…: {file_card_name}")
                                print(f"   - ì¹´í…Œê³ ë¦¬: {relative_path}")
                                return filepath
                                
                    except Exception as e:
                        print(f"â— JSON íŒŒì¼ ì½ê¸° ì˜¤ë¥˜ {filename}: {e}")
                        continue
            
            print(f"ğŸ“Š '{os.path.basename(data_dir)}' ê²€ìƒ‰ ìš”ì•½:")
            print(f"   - ì´ ê²€ì‚¬í•œ JSON íŒŒì¼: {total_files_checked}ê°œ")
            print(f"   - ë°œê²¬ëœ ì¹´í…Œê³ ë¦¬: {categories_found}")

        self._show_available_cards_sample()
        
        return None
    
    def _show_available_cards_sample(self, max_samples: int = 5):
        """ì‹¤ì œ ì¡´ì¬í•˜ëŠ” ì¹´ë“œë“¤ì˜ ìƒ˜í”Œ ëª©ë¡ í‘œì‹œ (ë””ë²„ê¹…ìš©)"""
        found_cards = []
        
        for data_dir in self.data_dirs:
            if not os.path.exists(data_dir):
                continue
                
            for root, dirs, files in os.walk(data_dir):
                json_files = [f for f in files if f.endswith('.json')]
                
                for filename in json_files[:max_samples]:  # ìƒ˜í”Œë§Œ í™•ì¸
                    try:
                        filepath = os.path.join(root, filename)
                        with open(filepath, 'r', encoding='utf-8') as f:
                            data = json.load(f)
                            card_name = data.get("card_name")
                            if card_name:
                                category = os.path.relpath(root, data_dir)
                                found_cards.append((card_name, category))
                                print(f"   - '{card_name}' (ì¹´í…Œê³ ë¦¬: {category})")
                    except:
                        continue
                        
                if len(found_cards) >= max_samples:
                    break
            
            if len(found_cards) >= max_samples:
                break
        
        if not found_cards:
            print("   âŒ ì‚¬ìš© ê°€ëŠ¥í•œ ì¹´ë“œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        else:
            print(f"   ğŸ’¡ ì´ {len(found_cards)}ê°œ ìƒ˜í”Œ í‘œì‹œ (ì‹¤ì œë¡œëŠ” ë” ë§ì„ ìˆ˜ ìˆìŒ)")

    def _get_card_category_from_path(self, json_path: str) -> str:
        """JSON íŒŒì¼ ê²½ë¡œì—ì„œ ì¹´ë“œ ì¹´í…Œê³ ë¦¬ ì¶”ì¶œ"""
        if "ì‹ ìš©json" in json_path or "credit" in json_path.lower():
            return "credit"
        elif "ì²´í¬json" in json_path or "check" in json_path.lower():
            return "check"
        else:
            return "unknown"

    def _prepare_card_data(self, card_name: str) -> tuple[list[Document], FAISS, BM25Retriever]:
        """ì¹´ë“œë³„ ë¬¸ì„œ ë° ì¸ë±ìŠ¤ ì¤€ë¹„ (ì¹´í…Œê³ ë¦¬ë³„ ìºì‹± + ê°œë³„ ì¹´ë“œ í•„í„°ë§)"""
        print(f"ğŸ”§ '{card_name}' ì¹´ë“œ ë°ì´í„° ì¤€ë¹„ ì¤‘...")
        
        # ê°œë³„ ì¹´ë“œ ìºì‹œ í™•ì¸
        if card_name in self._document_cache:
            print("ğŸ’¾ ê°œë³„ ì¹´ë“œ ìºì‹œ ì‚¬ìš© ì¤‘...")
            return (
                self._document_cache[card_name],
                self._faiss_cache[card_name],
                self._bm25_cache[card_name]
            )

        # ì¹´ë“œ JSON íŒŒì¼ ì°¾ê¸°
        json_path = self._find_card_json_path(card_name)
        if not json_path:
            raise ValueError(f"'{card_name}' ì¹´ë“œì˜ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

        # ì¹´ë“œì˜ ì¹´í…Œê³ ë¦¬ íŒë³„
        category = self._get_card_category_from_path(json_path)

        # ì¹´í…Œê³ ë¦¬ë³„ ì„ë² ë”©ì´ ìˆëŠ”ì§€ í™•ì¸í•˜ê³  ë¡œë“œ
        category_documents, category_faiss, category_bm25 = self._load_category_embeddings(category)
        
        if category_documents is None:
            print(f"âš ï¸ {category.upper()} ì¹´í…Œê³ ë¦¬ ì„ë² ë”©ì´ ì—†ìŠµë‹ˆë‹¤. ê°œë³„ ì¹´ë“œë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤...")
            return self._prepare_individual_card_data(card_name, json_path)
        
        # í•´ë‹¹ ì¹´ë“œì˜ ë¬¸ì„œë§Œ ì¶”ì¶œ
        card_documents = [doc for doc in category_documents if doc.metadata.get('card_name') == card_name]
        
        if not card_documents:
            print(f"âŒ {category.upper()} ì¹´í…Œê³ ë¦¬ì—ì„œ '{card_name}' ì¹´ë“œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            raise ValueError(f"ì¹´í…Œê³ ë¦¬ ì„ë² ë”©ì— '{card_name}' ì¹´ë“œê°€ ì—†ìŠµë‹ˆë‹¤.")

        card_faiss = FAISS.from_documents(card_documents, self.embedding_model)
        card_bm25 = BM25Retriever.from_documents(card_documents)
        card_bm25.k = 60

        # ê°œë³„ ì¹´ë“œ ìºì‹œ ì €ì¥
        self._document_cache[card_name] = card_documents
        self._faiss_cache[card_name] = card_faiss
        self._bm25_cache[card_name] = card_bm25

        return card_documents, card_faiss, card_bm25

    def _prepare_individual_card_data(self, card_name: str, json_path: str) -> tuple[list[Document], FAISS, BM25Retriever]:
        """ê°œë³„ ì¹´ë“œ ë°ì´í„° ì¤€ë¹„ (ê¸°ì¡´ ë°©ì‹)"""
        documents = self.load_documents_field_level([json_path])
        
        if not documents:
            raise ValueError(f"'{card_name}' ì¹´ë“œì˜ ë¬¸ì„œê°€ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤.")

        faiss_index = FAISS.from_documents(documents, self.embedding_model)
        bm25 = BM25Retriever.from_documents(documents)
        bm25.k = 60
        return documents, faiss_index, bm25

    def query(self, card_name: str, card_text: str, question: str, explain_easy: bool = False, top_k: int = 20) -> str:
        """
        ì¹´ë“œì— ëŒ€í•œ ì§ˆì˜ì‘ë‹µ ìˆ˜í–‰ (card_generator.py í˜¸í™˜ìš© ì¸í„°í˜ì´ìŠ¤)
        
        Args:
            card_name: ì¹´ë“œ ì´ë¦„ (selected_cards.jsonì—ì„œ ê°€ì ¸ì˜¨ ê°’)
            card_text: ì¹´ë“œ í…ìŠ¤íŠ¸ (ì‚¬ìš©ë˜ì§€ ì•ŠìŒ, í˜¸í™˜ì„±ì„ ìœ„í•´ ìœ ì§€)
            question: ì‚¬ìš©ì ì§ˆë¬¸
            explain_easy: ì‰½ê²Œ ì„¤ëª…í• ì§€ ì—¬ë¶€
            top_k: ìƒìœ„ kê°œ ë¬¸ì„œ ì‚¬ìš©
            
        Returns:
            ë‹µë³€ í…ìŠ¤íŠ¸
        """
        print(f"\n{'='*60}")
        print(f"ğŸš€ ì§ˆì˜ì‘ë‹µ ì‹œì‘")
        print(f"   - ì¹´ë“œëª…: {card_name}")
        print(f"   - ì§ˆë¬¸: {question}")
        print(f"   - ì‰¬ìš´ ì„¤ëª…: {explain_easy}")
        print(f"   - Top-K: {top_k}")
        print(f"{'='*60}")
        
        try:
            print(f"ğŸ” '{card_name}' ì¹´ë“œ ì •ë³´ ê²€ìƒ‰ ì¤‘...")
            
            # ì¹´ë“œ ë°ì´í„° ì¤€ë¹„
            documents, faiss_index, bm25 = self._prepare_card_data(card_name)

            print("ğŸ“Š ë¬¸ì„œ ê²€ìƒ‰ ë° ë­í‚¹ ì¤‘...")
            # FAISS ê²€ìƒ‰
            print("   - FAISS ìœ ì‚¬ë„ ê²€ìƒ‰ ì‹¤í–‰...")
            faiss_results = faiss_index.similarity_search(question, k=60)
            print(f"   - FAISS ê²°ê³¼: {len(faiss_results)}ê°œ")
            
            # BM25 ê²€ìƒ‰
            print("   - BM25 ê²€ìƒ‰ ì‹¤í–‰...")
            bm25_results = bm25.get_relevant_documents(question)
            print(f"   - BM25 ê²°ê³¼: {len(bm25_results)}ê°œ")
            
            # RRFë¡œ ê²°ê³¼ ìœµí•©
            rrf_candidates = self.reciprocal_rank_fusion(faiss_results, bm25_results)
            
            # Cross-Encoderë¡œ ì¬ë­í‚¹
            print(f"ğŸ”„ Cross-Encoder ì¬ë­í‚¹ ì‹¤í–‰... (ì´ {len(rrf_candidates)}ê°œ í›„ë³´)")
            inputs = [(question, chunk) for chunk in rrf_candidates]
            scores = self.reranker.predict(inputs)
            reranked_chunks = [x for _, x in sorted(zip(scores, rrf_candidates), reverse=True)]
            top_chunks = reranked_chunks[:top_k]
            print(f"âœ… ì¬ë­í‚¹ ì™„ë£Œ - ìµœì¢… {len(top_chunks)}ê°œ ì²­í¬ ì„ íƒ")

            # LangGraphë¡œ ë‹µë³€ ìƒì„±
            print("ğŸ’­ LangGraph ë‹µë³€ ìƒì„± ì‹œì‘...")
            initial_state = {
                "card_name": card_name,
                "user_question": question,
                "context_chunks": top_chunks,
                "prompt": "",
                "answer": "",
                "simplified_answer": "",
                "explain_easy": explain_easy
            }

            result = self.graph.invoke(initial_state)
            
            # ì‰¬ìš´ ì„¤ëª…ì´ ìš”ì²­ë˜ì—ˆê³  ìƒì„±ë˜ì—ˆë‹¤ë©´ ê·¸ê²ƒì„ ë°˜í™˜
            final_answer = ""
            if explain_easy and result["simplified_answer"]:
                final_answer = result["simplified_answer"]
                print("ğŸ“ ì‰¬ìš´ ì„¤ëª… ë²„ì „ ë°˜í™˜")
            else:
                final_answer = result["answer"]
                print("ğŸ“ ê¸°ë³¸ ë‹µë³€ ë²„ì „ ë°˜í™˜")
            
            print(f"ğŸ‰ ì§ˆì˜ì‘ë‹µ ì™„ë£Œ! (ìµœì¢… ë‹µë³€ ê¸¸ì´: {len(final_answer)}ì)")
            return final_answer

        except Exception as e:
            error_msg = f"âŒ '{card_name}' ì¹´ë“œ ì§ˆì˜ì‘ë‹µ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
            print(error_msg)
            return error_msg

    def clear_cache(self):
        """ìºì‹œëœ ë°ì´í„° í´ë¦¬ì–´"""
        print("ğŸ—‘ï¸ ìºì‹œ í´ë¦¬ì–´ ì¤‘...")
        self._document_cache.clear()
        self._faiss_cache.clear()
        self._bm25_cache.clear()
        print("âœ… ìºì‹œê°€ í´ë¦¬ì–´ë˜ì—ˆìŠµë‹ˆë‹¤.")

    def list_available_embeddings(self):
        """ì‚¬ìš© ê°€ëŠ¥í•œ ì„ë² ë”© íŒŒì¼ ëª©ë¡ í‘œì‹œ"""
        print(f"ğŸ“ ì„ë² ë”© ë””ë ‰í† ë¦¬: {self.embeddings_dir}")
        
        if not os.path.exists(self.embeddings_dir):
            print("âŒ ì„ë² ë”© ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
            return
        
        files = os.listdir(self.embeddings_dir)
        
        if not files:
            print("ğŸ“‚ ì„ë² ë”© íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        print("ğŸ“‹ ë°œê²¬ëœ ì„ë² ë”© íŒŒì¼:")
        for file in sorted(files):
            filepath = os.path.join(self.embeddings_dir, file)
            size = os.path.getsize(filepath) / (1024 * 1024)  # MB
            print(f"   - {file} ({size:.2f} MB)")


if __name__ == "__main__":
    print("ğŸš€ FAISSRAGRetriever í…ŒìŠ¤íŠ¸ ì‹œì‘")
    
    # í…ŒìŠ¤íŠ¸ ì½”ë“œ
    rag = FAISSRAGRetriever()
    
    # ì‚¬ìš© ê°€ëŠ¥í•œ ì„ë² ë”© í™•ì¸
    rag.list_available_embeddings()
    
    # ì¹´í…Œê³ ë¦¬ë³„ ì„ë² ë”© ë¹Œë“œ (ì²˜ìŒ ì‹¤í–‰ì‹œì—ë§Œ)
    build_embeddings = input("\nğŸ’¡ ì¹´í…Œê³ ë¦¬ë³„ ì„ë² ë”©ì„ ë¹Œë“œí•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/N): ").lower().strip()
    if build_embeddings in ['y', 'yes']:
        force_rebuild = input("ğŸ”„ ê¸°ì¡´ ì„ë² ë”©ì„ ê°•ì œë¡œ ì¬ë¹Œë“œí•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/N): ").lower().strip() in ['y', 'yes']
        rag.build_category_embeddings(force_rebuild=force_rebuild)
    
    # selected_cards.jsonì—ì„œ ë§ˆì§€ë§‰ ì¹´ë“œ ê°€ì ¸ì˜¤ê¸°
    latest_card = rag.get_latest_card_from_selected_cards()
    
    if latest_card:
        test_card = latest_card
        print(f"ğŸ¯ í…ŒìŠ¤íŠ¸í•  ì¹´ë“œ: {test_card}")
    else:
        test_card = "K-íŒ¨ìŠ¤ì¹´ë“œ"  # fallback
        print(f"âš ï¸ selected_cards.jsonì—ì„œ ì¹´ë“œë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ì–´ ê¸°ë³¸ê°’ ì‚¬ìš©: {test_card}")
    
    test_question = "ì´ ì¹´ë“œì˜ ì—°íšŒë¹„ëŠ” ì–¼ë§ˆì¸ê°€ìš”?"
    
    try:
        print(f"\nğŸ” ê¸°ë³¸ ë‹µë³€ í…ŒìŠ¤íŠ¸")
        answer = rag.query(card_name=test_card, card_text="", question=test_question)
        print(f"\nì§ˆë¬¸: {test_question}")
        print(f"ë‹µë³€: {answer}")
        
        print(f"\nğŸ” ì‰¬ìš´ ì„¤ëª… í…ŒìŠ¤íŠ¸")
        # ì‰¬ìš´ ì„¤ëª… í…ŒìŠ¤íŠ¸
        easy_answer = rag.query(card_name=test_card, card_text="", question=test_question, explain_easy=True)
        print(f"ì‰¬ìš´ ì„¤ëª…: {easy_answer}")
        
    except Exception as e:
        print(f"âŒ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
        import traceback
        traceback.print_exc()